{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def compute_similarity_histogram(img1_path, img2_path):\n",
    "    # Load images\n",
    "    img1 = cv2.imread(img1_path)\n",
    "    img2 = cv2.imread(img2_path)\n",
    "    if img1 is None or img2 is None:\n",
    "        raise ValueError(\"One or both image paths are invalid.\")\n",
    "    \n",
    "    # Convert images to HSV colour space\n",
    "    hsv1 = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV)\n",
    "    hsv2 = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Compute 2D histograms for H and S channels\n",
    "    hist1 = cv2.calcHist([hsv1], [0, 1], None, [50, 60], [0, 180, 0, 256])\n",
    "    hist2 = cv2.calcHist([hsv2], [0, 1], None, [50, 60], [0, 180, 0, 256])\n",
    "    \n",
    "    # Normalise histograms\n",
    "    cv2.normalize(hist1, hist1, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "    cv2.normalize(hist2, hist2, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Compare histograms using correlation\n",
    "    similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
    "    return similarity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image1_path = \"image1.jpg\"\n",
    "    image2_path = \"image2.jpg\"\n",
    "    \n",
    "    sim_score = compute_similarity_histogram(image1_path, image2_path)\n",
    "    print(f\"Similarity Score: {sim_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First pass through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv5n model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\husan/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-3-1 Python-3.12.1 torch-2.5.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n",
      "Adding AutoShape... \n",
      "C:\\Users\\husan/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\husan/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\husan/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings...\n",
      "\n",
      "Cosine similarity between image1 and image2: 0.8229\n",
      "Cosine similarity between image1 and image3: 0.7536\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "# 1) LOAD YOLOv5n MODEL\n",
    "###############################################################################\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads the YOLOv5n model from Torch Hub and sets it to evaluation mode.\n",
    "    \"\"\"\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# 2) RECURSIVELY FLATTEN ALL SUBMODULES\n",
    "###############################################################################\n",
    "def flatten_submodules(module: nn.Module):\n",
    "    \"\"\"\n",
    "    Recursively retrieve all submodules from a PyTorch module (BFS or DFS).\n",
    "    Returns them as a list in the order they were visited.\n",
    "\n",
    "    Example usage:\n",
    "      all_layers = flatten_submodules(model)\n",
    "      # then all_layers[0] is the first submodule, all_layers[1] is the second, etc.\n",
    "\n",
    "    If you need to see each layer's name, you can adapt this to return (name, layer).\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "\n",
    "    def recurse(m: nn.Module):\n",
    "        # Add this module itself\n",
    "        layers.append(m)\n",
    "        # Recurse on children\n",
    "        for child in m.children():\n",
    "            recurse(child)\n",
    "\n",
    "    recurse(module)\n",
    "    return layers\n",
    "\n",
    "###############################################################################\n",
    "# 3) GET EMBEDDING BY HOOKING A SUBMODULE\n",
    "###############################################################################\n",
    "def get_embedding(model, image_path: str, layer_index: int):\n",
    "    \"\"\"\n",
    "    Extracts a feature embedding from YOLOv5n by hooking into a specified\n",
    "    submodule index from a flattened list of submodules.\n",
    "    \"\"\"\n",
    "    # Load image via OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image not found: {image_path}\")\n",
    "\n",
    "    # Convert BGR -> RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Flatten all submodules of YOLO\n",
    "    all_layers = flatten_submodules(model)\n",
    "\n",
    "    if layer_index < 0 or layer_index >= len(all_layers):\n",
    "        msg = (\n",
    "            f\"Invalid layer_index={layer_index}. Valid range: 0..{len(all_layers)-1}\\n\\n\"\n",
    "            \"Hint: Inspect the layers or reduce your chosen index.\"\n",
    "        )\n",
    "        raise IndexError(msg)\n",
    "\n",
    "    # Hook function to capture output from the chosen layer\n",
    "    features = []\n",
    "    def hook_fn(module, input, output):\n",
    "        features.append(output)\n",
    "\n",
    "    chosen_layer = all_layers[layer_index]\n",
    "    hook = chosen_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Run inference on a single image\n",
    "    _ = model([img])\n",
    "\n",
    "    # Remove hook\n",
    "    hook.remove()\n",
    "\n",
    "    if not features:\n",
    "        raise RuntimeError(\"Feature extraction failed: The chosen layer produced no output.\")\n",
    "\n",
    "    # Flatten e.g. [batch, channels, H, W] -> [batch, channels*H*W]\n",
    "    feat = features[0].view(features[0].size(0), -1)\n",
    "    # Convert to NumPy (we only had one image in the batch)\n",
    "    vec = feat.detach().cpu().numpy()[0]\n",
    "    return vec\n",
    "\n",
    "###############################################################################\n",
    "# 4) COSINE SIMILARITY\n",
    "###############################################################################\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors.\n",
    "    Returns a float in [-1..1]. Closer to +1 => more similar.\n",
    "    \"\"\"\n",
    "    dot = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return float(dot / (norm1 * norm2))\n",
    "\n",
    "###############################################################################\n",
    "# 5) MAIN DEMO\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Hardcoded paths to images\n",
    "    image1_path = \"image1.jpg\"\n",
    "    image2_path = \"image2.jpg\"\n",
    "    image3_path = \"image3.jpg\"\n",
    "\n",
    "    # CHOOSE A LAYER INDEX\n",
    "    # Because we flatten the entire model, you can choose a wide range:\n",
    "    # e.g. 0, 1, 2 ... up to len(all_layers)-1. The deeper layers tend\n",
    "    # to produce more \"semantic\" features. Start with something like 50\n",
    "    # or 100, depending on the total number of submodules.\n",
    "    layer_index = 50\n",
    "\n",
    "    print(\"Loading YOLOv5n model...\")\n",
    "    model = load_model()\n",
    "\n",
    "    print(\"Extracting embeddings...\")\n",
    "    try:\n",
    "        vec1 = get_embedding(model, image1_path, layer_index)\n",
    "        vec2 = get_embedding(model, image2_path, layer_index)\n",
    "        vec3 = get_embedding(model, image3_path, layer_index)\n",
    "    except IndexError as e:\n",
    "        print(\"\\n[ERROR] Invalid layer index chosen.\")\n",
    "        print(e)\n",
    "        # Optional: print submodule count or submodule names here\n",
    "        # all_layers = flatten_submodules(model)\n",
    "        # for i, layer in enumerate(all_layers):\n",
    "        #     print(i, layer)\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "        return\n",
    "\n",
    "    # Compute similarities\n",
    "    sim_1_2 = cosine_similarity(vec1, vec2)\n",
    "    sim_1_3 = cosine_similarity(vec1, vec3)\n",
    "\n",
    "    print(f\"\\nCosine similarity between image1 and image2: {sim_1_2:.4f}\")\n",
    "    print(f\"Cosine similarity between image1 and image3: {sim_1_3:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second pass through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv5n model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\husan/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-3-1 Python-3.12.1 torch-2.5.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n",
      "Adding AutoShape... \n",
      "C:\\Users\\husan/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\husan/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings...\n",
      "\n",
      "Cosine similarity between image1 and image2: 0.8229\n",
      "Cosine similarity between image1 and image3: 0.7536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\husan/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "# 1) LOAD YOLOv5n MODEL\n",
    "###############################################################################\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads the YOLOv5n model from Torch Hub and sets it to evaluation mode.\n",
    "    \"\"\"\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# 2) RECURSIVELY FLATTEN ALL SUBMODULES\n",
    "###############################################################################\n",
    "def flatten_submodules(module: nn.Module):\n",
    "    \"\"\"\n",
    "    Recursively retrieve all submodules from a PyTorch module (BFS or DFS).\n",
    "    Returns them as a list in the order they were visited.\n",
    "\n",
    "    Example usage:\n",
    "      all_layers = flatten_submodules(model)\n",
    "      # then all_layers[0] is the first submodule, all_layers[1] is the second, etc.\n",
    "\n",
    "    If you need to see each layer's name, you can adapt this to return (name, layer).\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "\n",
    "    def recurse(m: nn.Module):\n",
    "        # Add this module itself\n",
    "        layers.append(m)\n",
    "        # Recurse on children\n",
    "        for child in m.children():\n",
    "            recurse(child)\n",
    "\n",
    "    recurse(module)\n",
    "    return layers\n",
    "\n",
    "###############################################################################\n",
    "# 3) GET EMBEDDING BY HOOKING A SUBMODULE\n",
    "###############################################################################\n",
    "def get_embedding(model, image_path: str, layer_index: int):\n",
    "    \"\"\"\n",
    "    Extracts a feature embedding from YOLOv5n by hooking into a specified\n",
    "    submodule index from a flattened list of submodules.\n",
    "    \"\"\"\n",
    "    # Load image via OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image not found: {image_path}\")\n",
    "\n",
    "    # Convert BGR -> RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Flatten all submodules of YOLO\n",
    "    all_layers = flatten_submodules(model)\n",
    "\n",
    "    if layer_index < 0 or layer_index >= len(all_layers):\n",
    "        msg = (\n",
    "            f\"Invalid layer_index={layer_index}. Valid range: 0..{len(all_layers)-1}\\n\\n\"\n",
    "            \"Hint: Inspect the layers or reduce your chosen index.\"\n",
    "        )\n",
    "        raise IndexError(msg)\n",
    "\n",
    "    # Hook function to capture output from the chosen layer\n",
    "    features = []\n",
    "    def hook_fn(module, input, output):\n",
    "        features.append(output)\n",
    "\n",
    "    chosen_layer = all_layers[layer_index]\n",
    "    hook = chosen_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Run inference on a single image\n",
    "    _ = model([img])\n",
    "\n",
    "    # Remove hook\n",
    "    hook.remove()\n",
    "\n",
    "    if not features:\n",
    "        raise RuntimeError(\"Feature extraction failed: The chosen layer produced no output.\")\n",
    "\n",
    "    # Flatten e.g. [batch, channels, H, W] -> [batch, channels*H*W]\n",
    "    feat = features[0].view(features[0].size(0), -1)\n",
    "    # Convert to NumPy (we only had one image in the batch)\n",
    "    vec = feat.detach().cpu().numpy()[0]\n",
    "    return vec\n",
    "\n",
    "###############################################################################\n",
    "# 4) COSINE SIMILARITY\n",
    "###############################################################################\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors.\n",
    "    Returns a float in [-1..1]. Closer to +1 => more similar.\n",
    "    \"\"\"\n",
    "    dot = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return float(dot / (norm1 * norm2))\n",
    "\n",
    "###############################################################################\n",
    "# 5) MAIN DEMO\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Hardcoded paths to images\n",
    "    image1_path = \"image1.jpg\"\n",
    "    image2_path = \"image2.jpg\"\n",
    "    image3_path = \"image3.jpg\"\n",
    "\n",
    "    # CHOOSE A LAYER INDEX\n",
    "    # Because we flatten the entire model, you can choose a wide range:\n",
    "    # e.g. 0, 1, 2 ... up to len(all_layers)-1. The deeper layers tend\n",
    "    # to produce more \"semantic\" features. Start with something like 50\n",
    "    # or 100, depending on the total number of submodules.\n",
    "    layer_index = 50\n",
    "\n",
    "    print(\"Loading YOLOv5n model...\")\n",
    "    model = load_model()\n",
    "\n",
    "    print(\"Extracting embeddings...\")\n",
    "    try:\n",
    "        vec1 = get_embedding(model, image1_path, layer_index)\n",
    "        vec2 = get_embedding(model, image2_path, layer_index)\n",
    "        vec3 = get_embedding(model, image3_path, layer_index)\n",
    "    except IndexError as e:\n",
    "        print(\"\\n[ERROR] Invalid layer index chosen.\")\n",
    "        print(e)\n",
    "        # Optional: print submodule count or submodule names here\n",
    "        # all_layers = flatten_submodules(model)\n",
    "        # for i, layer in enumerate(all_layers):\n",
    "        #     print(i, layer)\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "        return\n",
    "\n",
    "    # Compute similarities\n",
    "    sim_1_2 = cosine_similarity(vec1, vec2)\n",
    "    sim_1_3 = cosine_similarity(vec1, vec3)\n",
    "\n",
    "    print(f\"\\nCosine similarity between image1 and image2: {sim_1_2:.4f}\")\n",
    "    print(f\"Cosine similarity between image1 and image3: {sim_1_3:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
